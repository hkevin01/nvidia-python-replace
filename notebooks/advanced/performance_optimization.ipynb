{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "89f30a39",
   "metadata": {},
   "source": [
    "# Advanced Performance Optimization with GPU Acceleration\n",
    "\n",
    "This notebook demonstrates advanced techniques for optimizing GPU-accelerated data processing:\n",
    "\n",
    "1. Memory management and data transfer optimization\n",
    "2. Batch processing strategies\n",
    "3. Multi-GPU utilization\n",
    "4. Pipeline optimization\n",
    "5. Performance profiling and benchmarking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16322044",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cudf\n",
    "import cupy as cp\n",
    "import numpy as np\n",
    "from time import time\n",
    "import psutil\n",
    "import os\n",
    "\n",
    "# Helper function for memory usage\n",
    "def get_memory_usage():\n",
    "    process = psutil.Process(os.getpid())\n",
    "    return {\n",
    "        'RAM': f\"{process.memory_info().rss / 1024 / 1024:.2f} MB\",\n",
    "        'GPU': f\"{cp.get_default_memory_pool().used_bytes() / 1024 / 1024:.2f} MB\"\n",
    "    }\n",
    "\n",
    "print(\"Initial memory usage:\")\n",
    "print(get_memory_usage())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ea66ee9",
   "metadata": {},
   "source": [
    "## Memory Management and Data Transfer\n",
    "\n",
    "Let's explore efficient memory management and data transfer strategies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e71f9f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a large dataset\n",
    "n_rows = 10_000_000\n",
    "n_cols = 10\n",
    "\n",
    "print(\"Creating large dataset...\")\n",
    "print(\"Before creation:\", get_memory_usage())\n",
    "\n",
    "# Inefficient way - create all columns at once\n",
    "start = time()\n",
    "data = {f'col_{i}': np.random.randn(n_rows) for i in range(n_cols)}\n",
    "df_inefficient = cudf.DataFrame(data)\n",
    "print(f\"\\nInefficient creation time: {time() - start:.2f} seconds\")\n",
    "print(\"After inefficient creation:\", get_memory_usage())\n",
    "\n",
    "# Clear memory\n",
    "del df_inefficient\n",
    "cp.get_default_memory_pool().free_all_blocks()\n",
    "\n",
    "# Efficient way - stream columns one by one\n",
    "start = time()\n",
    "df_efficient = cudf.DataFrame()\n",
    "for i in range(n_cols):\n",
    "    df_efficient[f'col_{i}'] = cudf.Series(np.random.randn(n_rows))\n",
    "    \n",
    "print(f\"\\nEfficient creation time: {time() - start:.2f} seconds\")\n",
    "print(\"After efficient creation:\", get_memory_usage())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8898a877",
   "metadata": {},
   "source": [
    "## Batch Processing Strategies\n",
    "\n",
    "When dealing with very large datasets, batch processing can help manage memory usage and improve overall performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ab01281",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to process a batch\n",
    "def process_batch(df):\n",
    "    # Simulate complex processing\n",
    "    result = (\n",
    "        df.sum() + \n",
    "        df.mean() * \n",
    "        df.std()\n",
    "    )\n",
    "    return result\n",
    "\n",
    "# Process without batching\n",
    "start = time()\n",
    "result_no_batch = process_batch(df_efficient)\n",
    "print(f\"Processing without batching: {time() - start:.2f} seconds\")\n",
    "print(\"Memory after full processing:\", get_memory_usage())\n",
    "\n",
    "# Process with batching\n",
    "start = time()\n",
    "batch_size = len(df_efficient) // 4\n",
    "results = []\n",
    "\n",
    "for i in range(0, len(df_efficient), batch_size):\n",
    "    batch = df_efficient.iloc[i:i + batch_size]\n",
    "    result = process_batch(batch)\n",
    "    results.append(result)\n",
    "\n",
    "# Combine results\n",
    "result_batched = sum(results)\n",
    "print(f\"\\nProcessing with batching: {time() - start:.2f} seconds\")\n",
    "print(\"Memory after batch processing:\", get_memory_usage())\n",
    "\n",
    "# Verify results are similar\n",
    "print(\"\\nResults difference (should be small):\")\n",
    "print(abs(result_no_batch - result_batched).max())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de029d5b",
   "metadata": {},
   "source": [
    "## Pipeline Optimization\n",
    "\n",
    "Let's explore how to optimize data processing pipelines using GPU acceleration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe863123",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a sample pipeline\n",
    "def inefficient_pipeline(df):\n",
    "    # Perform operations one at a time\n",
    "    start = time()\n",
    "    \n",
    "    # Step 1: Standardize\n",
    "    df = (df - df.mean()) / df.std()\n",
    "    \n",
    "    # Step 2: Remove outliers\n",
    "    df = df[abs(df) < 3]\n",
    "    \n",
    "    # Step 3: Calculate rolling statistics\n",
    "    df = df.rolling(100).mean()\n",
    "    \n",
    "    return time() - start, df\n",
    "\n",
    "def optimized_pipeline(df):\n",
    "    # Combine operations to minimize memory transfers\n",
    "    start = time()\n",
    "    \n",
    "    # Calculate statistics once\n",
    "    means = df.mean()\n",
    "    stds = df.std()\n",
    "    \n",
    "    # Perform operations in a single pass\n",
    "    df = (\n",
    "        (df - means) / stds\n",
    "    ).pipe(\n",
    "        lambda x: x[abs(x) < 3]\n",
    "    ).pipe(\n",
    "        lambda x: x.rolling(100).mean()\n",
    "    )\n",
    "    \n",
    "    return time() - start, df\n",
    "\n",
    "# Compare pipelines\n",
    "inefficient_time, inefficient_result = inefficient_pipeline(df_efficient)\n",
    "optimized_time, optimized_result = optimized_pipeline(df_efficient)\n",
    "\n",
    "print(f\"Inefficient pipeline time: {inefficient_time:.2f} seconds\")\n",
    "print(f\"Optimized pipeline time: {optimized_time:.2f} seconds\")\n",
    "print(f\"Speedup: {inefficient_time / optimized_time:.2f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c79fa59",
   "metadata": {},
   "source": [
    "## Performance Profiling\n",
    "\n",
    "Let's profile our GPU operations to identify bottlenecks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1b14ab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def profile_operation(func, *args, **kwargs):\n",
    "    # Record start state\n",
    "    start_mem = get_memory_usage()\n",
    "    start_time = time()\n",
    "    \n",
    "    # Run operation\n",
    "    result = func(*args, **kwargs)\n",
    "    \n",
    "    # Record end state\n",
    "    end_time = time()\n",
    "    end_mem = get_memory_usage()\n",
    "    \n",
    "    # Calculate changes\n",
    "    time_taken = end_time - start_time\n",
    "    ram_change = float(end_mem['RAM'].split()[0]) - float(start_mem['RAM'].split()[0])\n",
    "    gpu_change = float(end_mem['GPU'].split()[0]) - float(start_mem['GPU'].split()[0])\n",
    "    \n",
    "    print(f\"Operation took {time_taken:.2f} seconds\")\n",
    "    print(f\"RAM change: {ram_change:+.2f} MB\")\n",
    "    print(f\"GPU memory change: {gpu_change:+.2f} MB\")\n",
    "    return result\n",
    "\n",
    "# Profile some operations\n",
    "print(\"Profiling DataFrame creation:\")\n",
    "profile_operation(cudf.DataFrame, {'a': np.random.randn(1000000)})\n",
    "\n",
    "print(\"\\nProfiling complex calculation:\")\n",
    "profile_operation(\n",
    "    lambda df: (df['a']**2 + df['a'].mean()) * df['a'].std(),\n",
    "    cudf.DataFrame({'a': np.random.randn(1000000)})\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62515797",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this notebook, we've explored several advanced performance optimization techniques for GPU-accelerated computing:\n",
    "\n",
    "1. Memory management and efficient data transfer strategies\n",
    "2. Batch processing for handling large datasets\n",
    "3. Pipeline optimization to minimize memory transfers\n",
    "4. Performance profiling to identify bottlenecks\n",
    "\n",
    "These techniques are essential for building efficient, scalable data processing systems with GPU acceleration."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
