{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b5d9b9ea",
   "metadata": {},
   "source": [
    "# Advanced Specialized Techniques with GPU Acceleration\n",
    "\n",
    "This notebook demonstrates specialized techniques using GPU acceleration, including:\n",
    "1. Graph analytics with cuGraph\n",
    "2. Clustering with HDBSCAN\n",
    "3. Dimensionality reduction with UMAP\n",
    "4. Advanced time series analysis with cuDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dd3c9b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cudf\n",
    "import cugraph\n",
    "import cuml\n",
    "import numpy as np\n",
    "from time import time\n",
    "import umap\n",
    "import hdbscan\n",
    "from sklearn.datasets import make_blobs\n",
    "\n",
    "# Create a synthetic dataset for clustering\n",
    "n_samples = 100000\n",
    "n_features = 50\n",
    "centers = 5\n",
    "\n",
    "X, y = make_blobs(\n",
    "    n_samples=n_samples, \n",
    "    n_features=n_features,\n",
    "    centers=centers,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Convert to cuDF DataFrame\n",
    "df_gpu = cudf.DataFrame(X)\n",
    "print(f\"Created dataset with {n_samples:,} samples and {n_features} features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34780a2e",
   "metadata": {},
   "source": [
    "## Dimensionality Reduction with UMAP\n",
    "\n",
    "UMAP (Uniform Manifold Approximation and Projection) is a powerful dimensionality reduction technique. Let's use it to reduce our high-dimensional data to 2D for visualization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43422e97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize and fit UMAP\n",
    "start = time()\n",
    "umap_reducer = umap.UMAP(n_neighbors=15, min_dist=0.1, random_state=42)\n",
    "X_umap = umap_reducer.fit_transform(X)\n",
    "\n",
    "print(f\"UMAP reduction completed in {time() - start:.2f} seconds\")\n",
    "\n",
    "# Convert reduced data to cuDF DataFrame\n",
    "df_reduced = cudf.DataFrame(X_umap, columns=['UMAP1', 'UMAP2'])\n",
    "print(\"\\nReduced data shape:\", df_reduced.shape)\n",
    "print(\"\\nFirst few rows of reduced data:\")\n",
    "print(df_reduced.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de94e024",
   "metadata": {},
   "source": [
    "## Clustering with HDBSCAN\n",
    "\n",
    "Now let's use HDBSCAN to perform density-based clustering on our reduced data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "584301e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize and fit HDBSCAN\n",
    "start = time()\n",
    "clusterer = hdbscan.HDBSCAN(\n",
    "    min_cluster_size=50,\n",
    "    min_samples=5,\n",
    "    prediction_data=True\n",
    ")\n",
    "cluster_labels = clusterer.fit_predict(X_umap)\n",
    "\n",
    "print(f\"HDBSCAN clustering completed in {time() - start:.2f} seconds\")\n",
    "\n",
    "# Add cluster labels to our data\n",
    "df_reduced['cluster'] = cluster_labels\n",
    "print(\"\\nUnique clusters found:\", len(np.unique(cluster_labels)))\n",
    "print(\"\\nCluster distribution:\")\n",
    "print(df_reduced['cluster'].value_counts().to_pandas())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89c263f9",
   "metadata": {},
   "source": [
    "## Graph Analytics with cuGraph\n",
    "\n",
    "Let's create a graph based on our data points and analyze it using cuGraph:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f86b6c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create edges between points based on proximity\n",
    "start = time()\n",
    "\n",
    "# Use UMAP coordinates to create edges\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "nbrs = NearestNeighbors(n_neighbors=5).fit(X_umap)\n",
    "distances, indices = nbrs.kneighbors(X_umap)\n",
    "\n",
    "# Create edge list\n",
    "source_nodes = np.repeat(np.arange(len(X_umap)), 5)\n",
    "target_nodes = indices.ravel()\n",
    "weights = distances.ravel()\n",
    "\n",
    "# Create cuGraph DataFrame\n",
    "edges_df = cudf.DataFrame({\n",
    "    'source': source_nodes,\n",
    "    'destination': target_nodes,\n",
    "    'weight': weights\n",
    "})\n",
    "\n",
    "# Create graph\n",
    "G = cugraph.Graph()\n",
    "G.from_cudf_edgelist(edges_df, source='source', destination='destination', edge_attr='weight')\n",
    "\n",
    "# Calculate PageRank\n",
    "pagerank_scores = cugraph.pagerank(G)\n",
    "\n",
    "print(f\"Graph creation and PageRank calculation completed in {time() - start:.2f} seconds\")\n",
    "print(\"\\nPageRank scores summary:\")\n",
    "print(pagerank_scores.describe().to_pandas())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d1718a3",
   "metadata": {},
   "source": [
    "## Advanced Time Series Analysis\n",
    "\n",
    "Let's create and analyze a time series dataset using GPU acceleration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50448317",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create time series data\n",
    "start = time()\n",
    "n_timestamps = 1000000\n",
    "dates = pd.date_range('2020-01-01', periods=n_timestamps, freq='1min')\n",
    "values = np.sin(np.arange(n_timestamps) * 2 * np.pi / 1440) + np.random.normal(0, 0.1, n_timestamps)\n",
    "\n",
    "# Create cuDF DataFrame\n",
    "ts_df = cudf.DataFrame({\n",
    "    'timestamp': dates,\n",
    "    'value': values\n",
    "})\n",
    "\n",
    "# Calculate rolling statistics\n",
    "window_size = 60  # 1 hour window\n",
    "ts_df['rolling_mean'] = ts_df['value'].rolling(window_size).mean()\n",
    "ts_df['rolling_std'] = ts_df['value'].rolling(window_size).std()\n",
    "ts_df['rolling_zscore'] = (ts_df['value'] - ts_df['rolling_mean']) / ts_df['rolling_std']\n",
    "\n",
    "# Find anomalies (z-score > 3)\n",
    "anomalies = ts_df[abs(ts_df['rolling_zscore']) > 3]\n",
    "\n",
    "print(f\"Time series analysis completed in {time() - start:.2f} seconds\")\n",
    "print(f\"\\nFound {len(anomalies)} anomalies in {n_timestamps:,} data points\")\n",
    "print(\"\\nSample of detected anomalies:\")\n",
    "print(anomalies.head().to_pandas())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bddf803a",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this notebook, we've explored several advanced specialized techniques using GPU acceleration:\n",
    "\n",
    "1. Dimensionality reduction with UMAP for visualizing high-dimensional data\n",
    "2. Density-based clustering with HDBSCAN\n",
    "3. Graph analytics using cuGraph, including PageRank calculations\n",
    "4. Advanced time series analysis with rolling statistics and anomaly detection\n",
    "\n",
    "These techniques demonstrate the power of GPU acceleration for complex data analysis tasks, showing significant performance improvements over CPU-based implementations."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
